{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee8b12d7",
   "metadata": {},
   "source": [
    "# Fish Tracking with SAM2 (Segment Anything Model 2)\n",
    "\n",
    "This notebook implements fish tracking and annotation using Meta's SAM2 model.\n",
    "Designed to run on Haifa University server infrastructure.\n",
    "\n",
    "## Overview\n",
    "- Interactive fish annotation and tracking\n",
    "- Video processing and segmentation\n",
    "- Export annotations for training/analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ddc0f",
   "metadata": {},
   "source": [
    "## 1. Install and Import Dependencies\n",
    "\n",
    "First, we'll install SAM2 and all required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1459ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SAM2 from GitHub\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything-2.git\n",
    "\n",
    "# Install additional dependencies\n",
    "!pip install opencv-python matplotlib numpy torch torchvision pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# SAM2 imports\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76358e",
   "metadata": {},
   "source": [
    "## 2. Configure SAM2 Model\n",
    "\n",
    "Download and configure SAM2 model checkpoints. We'll use the large model for best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9deebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration paths\n",
    "MODEL_DIR = \"../models\"\n",
    "DATA_DIR = \"../data\"\n",
    "OUTPUT_DIR = \"../outputs\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model configuration\n",
    "# Options: sam2_hiera_tiny, sam2_hiera_small, sam2_hiera_base_plus, sam2_hiera_large\n",
    "MODEL_CFG = \"sam2_hiera_large.yaml\"\n",
    "CHECKPOINT = \"sam2_hiera_large.pt\"\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SAM2 checkpoint if not already present\n",
    "import urllib.request\n",
    "\n",
    "checkpoint_path = os.path.join(MODEL_DIR, CHECKPOINT)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"Downloading {CHECKPOINT}...\")\n",
    "    url = f\"https://dl.fbaipublicfiles.com/segment_anything_2/072824/{CHECKPOINT}\"\n",
    "    urllib.request.urlretrieve(url, checkpoint_path)\n",
    "    print(\"Download complete!\")\n",
    "else:\n",
    "    print(f\"Checkpoint already exists at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb373d35",
   "metadata": {},
   "source": [
    "## 3. Load and Display Sample Fish Video/Images\n",
    "\n",
    "Load your fish video or image sequences from the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ff7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video processing function\n",
    "def extract_frames_from_video(video_path, output_dir, max_frames=None):\n",
    "    \"\"\"Extract frames from video file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if max_frames and frame_count >= max_frames:\n",
    "            break\n",
    "            \n",
    "        frame_path = os.path.join(output_dir, f\"frame_{frame_count:05d}.jpg\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {frame_count} frames to {output_dir}\")\n",
    "    return frame_count\n",
    "\n",
    "# Load frames from directory\n",
    "def load_frames(frames_dir):\n",
    "    \"\"\"Load all frames from directory\"\"\"\n",
    "    frame_files = sorted([f for f in os.listdir(frames_dir) if f.endswith(('.jpg', '.png'))])\n",
    "    frames = []\n",
    "    \n",
    "    for frame_file in frame_files:\n",
    "        frame_path = os.path.join(frames_dir, frame_file)\n",
    "        frame = cv2.imread(frame_path)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frames.append(frame)\n",
    "    \n",
    "    print(f\"Loaded {len(frames)} frames\")\n",
    "    return frames, frame_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b44b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract frames from video (uncomment and modify path)\n",
    "# VIDEO_PATH = os.path.join(DATA_DIR, \"fish_video.mp4\")\n",
    "# FRAMES_DIR = os.path.join(DATA_DIR, \"frames\")\n",
    "# extract_frames_from_video(VIDEO_PATH, FRAMES_DIR, max_frames=100)\n",
    "\n",
    "# Or load existing frames\n",
    "FRAMES_DIR = os.path.join(DATA_DIR, \"frames\")\n",
    "print(f\"Looking for frames in: {FRAMES_DIR}\")\n",
    "print(\"Note: Place your video frames in the '../data/frames' directory\")\n",
    "\n",
    "# Check if frames directory exists\n",
    "if os.path.exists(FRAMES_DIR) and len(os.listdir(FRAMES_DIR)) > 0:\n",
    "    frames, frame_files = load_frames(FRAMES_DIR)\n",
    "    \n",
    "    # Display first frame\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(frames[0])\n",
    "    plt.title(f\"First frame: {frame_files[0]}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No frames found. Please add video frames to the data/frames directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65d26be",
   "metadata": {},
   "source": [
    "## 4. Initialize SAM2 Video Predictor\n",
    "\n",
    "Load the SAM2 model and prepare it for video tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1743853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAM2 video predictor\n",
    "predictor = build_sam2_video_predictor(MODEL_CFG, checkpoint_path)\n",
    "\n",
    "print(\"✓ SAM2 video predictor initialized successfully!\")\n",
    "print(f\"Model: {MODEL_CFG}\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc81566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference state for video tracking\n",
    "if os.path.exists(FRAMES_DIR) and len(os.listdir(FRAMES_DIR)) > 0:\n",
    "    inference_state = predictor.init_state(video_path=FRAMES_DIR)\n",
    "    print(\"✓ Inference state initialized for video tracking\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping initialization - no frames available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4daad5ea",
   "metadata": {},
   "source": [
    "## 5. Add Tracking Points for Fish\n",
    "\n",
    "Define initial points for the fish you want to track. You can click on the fish or provide coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff105e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive point selection helper\n",
    "def show_points_on_image(image, points, labels):\n",
    "    \"\"\"Visualize points on image\"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    for i, (point, label) in enumerate(zip(points, labels)):\n",
    "        x, y = point\n",
    "        if label == 1:  # Positive point\n",
    "            plt.plot(x, y, 'go', markersize=10, markeredgewidth=2, markeredgecolor='white')\n",
    "        else:  # Negative point\n",
    "            plt.plot(x, y, 'ro', markersize=10, markeredgewidth=2, markeredgecolor='white')\n",
    "    \n",
    "    plt.title(\"Click points: Green = Foreground (fish), Red = Background\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Helper class for interactive point selection\n",
    "class PointSelector:\n",
    "    def __init__(self, image):\n",
    "        self.image = image\n",
    "        self.points = []\n",
    "        self.labels = []\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 8))\n",
    "        \n",
    "    def onclick(self, event):\n",
    "        if event.xdata is not None and event.ydata is not None:\n",
    "            x, y = int(event.xdata), int(event.ydata)\n",
    "            # Left click = positive (fish), right click = negative (background)\n",
    "            label = 1 if event.button == 1 else 0\n",
    "            self.points.append([x, y])\n",
    "            self.labels.append(label)\n",
    "            \n",
    "            # Plot point\n",
    "            color = 'g' if label == 1 else 'r'\n",
    "            self.ax.plot(x, y, f'{color}o', markersize=10, markeredgewidth=2, markeredgecolor='white')\n",
    "            self.fig.canvas.draw()\n",
    "            \n",
    "            print(f\"Added {'positive' if label == 1 else 'negative'} point at ({x}, {y})\")\n",
    "    \n",
    "    def select_points(self):\n",
    "        self.ax.imshow(self.image)\n",
    "        self.ax.set_title(\"Left click: Fish (foreground) | Right click: Background | Close window when done\")\n",
    "        self.ax.axis('off')\n",
    "        self.fig.canvas.mpl_connect('button_press_event', self.onclick)\n",
    "        plt.show()\n",
    "        \n",
    "        return np.array(self.points), np.array(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f7d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Manual point definition (modify these coordinates)\n",
    "# Or use interactive selection below\n",
    "\n",
    "# Manual point definition\n",
    "# points = np.array([[x1, y1], [x2, y2], ...])  # Coordinates of fish\n",
    "# labels = np.array([1, 1, ...])  # 1 = positive (fish), 0 = negative (background)\n",
    "\n",
    "# Interactive selection (uncomment to use)\n",
    "if os.path.exists(FRAMES_DIR) and len(os.listdir(FRAMES_DIR)) > 0:\n",
    "    print(\"Interactive point selection:\")\n",
    "    print(\"- Left click on the fish to add positive points\")\n",
    "    print(\"- Right click on background to add negative points\")\n",
    "    print(\"- Close the window when done\")\n",
    "    \n",
    "    selector = PointSelector(frames[0])\n",
    "    # Uncomment the next line to enable interactive selection\n",
    "    # points, labels = selector.select_points()\n",
    "    \n",
    "    # Example points for demonstration (replace with your own)\n",
    "    points = np.array([[320, 240]])  # Center of image - modify as needed\n",
    "    labels = np.array([1])\n",
    "    \n",
    "    show_points_on_image(frames[0], points, labels)\n",
    "else:\n",
    "    print(\"⚠️ No frames available for point selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61481802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add points to SAM2 for tracking\n",
    "if os.path.exists(FRAMES_DIR) and len(os.listdir(FRAMES_DIR)) > 0:\n",
    "    # Frame index to start tracking (usually 0 for first frame)\n",
    "    frame_idx = 0\n",
    "    \n",
    "    # Object ID for tracking (can track multiple objects with different IDs)\n",
    "    obj_id = 1\n",
    "    \n",
    "    # Add points to predictor\n",
    "    predictor.add_new_points(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=frame_idx,\n",
    "        obj_id=obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Added {len(points)} tracking point(s) for object {obj_id} at frame {frame_idx}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping - no frames available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a928d01",
   "metadata": {},
   "source": [
    "## 6. Propagate Annotations Across Frames\n",
    "\n",
    "Run SAM2 tracking to propagate the annotations across all video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf3b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propagate tracking through video\n",
    "if os.path.exists(FRAMES_DIR) and len(os.listdir(FRAMES_DIR)) > 0:\n",
    "    print(\"Propagating annotations across frames...\")\n",
    "    \n",
    "    # Run propagation\n",
    "    video_segments = {}  # Store segmentation results\n",
    "    \n",
    "    for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "        video_segments[out_frame_idx] = {\n",
    "            out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "            for i, out_obj_id in enumerate(out_obj_ids)\n",
    "        }\n",
    "    \n",
    "    print(f\"✓ Propagation complete! Tracked across {len(video_segments)} frames\")\n",
    "    print(f\"Objects tracked: {list(video_segments[0].keys())}\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping - no frames available\")\n",
    "    video_segments = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2872a1",
   "metadata": {},
   "source": [
    "## 7. Visualize Tracking Results\n",
    "\n",
    "Display the tracked fish with masks overlaid on video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badb50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helper functions\n",
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    \"\"\"Display a segmentation mask\"\"\"\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    \n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def visualize_frame_with_masks(frame, masks, title=\"Tracked Fish\"):\n",
    "    \"\"\"Visualize a frame with all object masks\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.imshow(frame)\n",
    "    \n",
    "    for obj_id, mask in masks.items():\n",
    "        show_mask(mask[0], ax, obj_id=obj_id)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a71c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results on sample frames\n",
    "if len(video_segments) > 0:\n",
    "    # Show results on first, middle, and last frames\n",
    "    sample_indices = [0, len(video_segments)//2, len(video_segments)-1]\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        if idx in video_segments:\n",
    "            visualize_frame_with_masks(\n",
    "                frames[idx], \n",
    "                video_segments[idx],\n",
    "                title=f\"Frame {idx}: Tracked Fish\"\n",
    "            )\n",
    "else:\n",
    "    print(\"⚠️ No tracking results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7640e9",
   "metadata": {},
   "source": [
    "## 8. Export Annotations\n",
    "\n",
    "Export tracking results to standard annotation formats for training or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c100120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def export_to_coco(video_segments, frames, output_path):\n",
    "    \"\"\"Export annotations in COCO format\"\"\"\n",
    "    coco_output = {\n",
    "        \"info\": {\n",
    "            \"description\": \"Fish Tracking with SAM2\",\n",
    "            \"date_created\": datetime.now().isoformat(),\n",
    "        },\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": 1, \"name\": \"fish\", \"supercategory\": \"animal\"}]\n",
    "    }\n",
    "    \n",
    "    annotation_id = 1\n",
    "    \n",
    "    for frame_idx, masks_dict in video_segments.items():\n",
    "        # Add image info\n",
    "        height, width = frames[frame_idx].shape[:2]\n",
    "        coco_output[\"images\"].append({\n",
    "            \"id\": frame_idx,\n",
    "            \"file_name\": f\"frame_{frame_idx:05d}.jpg\",\n",
    "            \"height\": height,\n",
    "            \"width\": width\n",
    "        })\n",
    "        \n",
    "        # Add annotations for each object\n",
    "        for obj_id, mask in masks_dict.items():\n",
    "            # Get bounding box from mask\n",
    "            mask_binary = mask[0].astype(np.uint8)\n",
    "            contours, _ = cv2.findContours(mask_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            \n",
    "            if contours:\n",
    "                x, y, w, h = cv2.boundingRect(contours[0])\n",
    "                area = int(mask_binary.sum())\n",
    "                \n",
    "                coco_output[\"annotations\"].append({\n",
    "                    \"id\": annotation_id,\n",
    "                    \"image_id\": frame_idx,\n",
    "                    \"category_id\": 1,\n",
    "                    \"bbox\": [int(x), int(y), int(w), int(h)],\n",
    "                    \"area\": area,\n",
    "                    \"segmentation\": [],  # Can add polygon points if needed\n",
    "                    \"iscrowd\": 0,\n",
    "                    \"track_id\": int(obj_id)\n",
    "                })\n",
    "                annotation_id += 1\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(coco_output, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Exported {len(coco_output['annotations'])} annotations to {output_path}\")\n",
    "    return coco_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aed867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export annotations\n",
    "if len(video_segments) > 0:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    coco_path = os.path.join(OUTPUT_DIR, f\"fish_annotations_{timestamp}.json\")\n",
    "    \n",
    "    coco_annotations = export_to_coco(video_segments, frames, coco_path)\n",
    "    print(f\"\\nAnnotation summary:\")\n",
    "    print(f\"- Total frames: {len(coco_annotations['images'])}\")\n",
    "    print(f\"- Total annotations: {len(coco_annotations['annotations'])}\")\n",
    "else:\n",
    "    print(\"⚠️ No annotations to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd0820",
   "metadata": {},
   "source": [
    "## 9. Save Tracked Masks\n",
    "\n",
    "Save segmentation masks to disk for future use and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490abe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save masks and overlays\n",
    "if len(video_segments) > 0:\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Create output directories\n",
    "    masks_dir = os.path.join(OUTPUT_DIR, f\"masks_{timestamp}\")\n",
    "    overlays_dir = os.path.join(OUTPUT_DIR, f\"overlays_{timestamp}\")\n",
    "    os.makedirs(masks_dir, exist_ok=True)\n",
    "    os.makedirs(overlays_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Saving masks and overlays...\")\n",
    "    \n",
    "    for frame_idx, masks_dict in video_segments.items():\n",
    "        # Save binary masks\n",
    "        for obj_id, mask in masks_dict.items():\n",
    "            mask_binary = (mask[0] * 255).astype(np.uint8)\n",
    "            mask_path = os.path.join(masks_dir, f\"frame_{frame_idx:05d}_obj_{obj_id}.png\")\n",
    "            cv2.imwrite(mask_path, mask_binary)\n",
    "        \n",
    "        # Save overlay visualization\n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        ax.imshow(frames[frame_idx])\n",
    "        for obj_id, mask in masks_dict.items():\n",
    "            show_mask(mask[0], ax, obj_id=obj_id)\n",
    "        ax.axis('off')\n",
    "        \n",
    "        overlay_path = os.path.join(overlays_dir, f\"frame_{frame_idx:05d}.png\")\n",
    "        plt.savefig(overlay_path, bbox_inches='tight', pad_inches=0, dpi=100)\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"✓ Saved masks to: {masks_dir}\")\n",
    "    print(f\"✓ Saved overlays to: {overlays_dir}\")\n",
    "    \n",
    "    # Save tracking metadata\n",
    "    metadata = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"num_frames\": len(video_segments),\n",
    "        \"num_objects\": len(list(video_segments.values())[0]) if video_segments else 0,\n",
    "        \"model\": MODEL_CFG,\n",
    "        \"masks_dir\": masks_dir,\n",
    "        \"overlays_dir\": overlays_dir\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(OUTPUT_DIR, f\"tracking_metadata_{timestamp}.json\")\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved metadata to: {metadata_path}\")\n",
    "else:\n",
    "    print(\"⚠️ No masks to save\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
